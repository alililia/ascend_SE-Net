{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 使用条件对抗网络的图像到图像转换—Pix2Pix\n",
    "\n",
    " `GPU` `进阶` `计算机视觉` `全流程`\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pix2Pix概述\n",
    "\n",
    "Pix2Pix是基于条件生成对抗网络（cGAN, Condition Generative Adversarial Networks ）实现的一种深度学习图像转换模型，该模型是由Phillip Isola等作者在2017年CVPR上提出的,可以实现语义/标签到真实图片、灰度图到彩色图、航空图到地图、白天到黑夜、线稿图到实物图的转换。Pix2Pix是将cGAN应用于有监督的图像到图像翻译的经典之作，其包括两个模型：**发生器**和**鉴别器**。cGAN的生成器与传统GAN的生成器在原理上有一些区别，cGAN的生成器是将输入图片作为指导信息，由输入图像不断尝试生成用于迷惑鉴别器的“假”图像，由输入图像转换输出为相应“假”图像的本质是从像素到另一个像素的映射，而传统GAN的生成器是基于一个给定的随机噪声生成图像，输出图像通过其他约束条件控制生成，这是cGAN和GAN的在图像翻译任务中的差异。Pix2Pix中鉴别器的任务是判断从生成器输出的图像是真实的训练图像还是生成的“假”图像。在生成器与鉴别器的不断博弈过程中，模型会达到一个平衡点，生成器输出的图像与真实训练数据使得鉴别器刚好具有50%的概率判断正确。\n",
    "\n",
    "在教程开始前，首先定义一些在整个过程中需要用到的符号：\n",
    "\n",
    "- $x$：代表观测图像的数据。\n",
    "- $z$：代表随机噪声的数据。\n",
    "- $y=G(x,z)$：生成器网络，给出由观测图像$x$与随机噪声$z$生成的“假”图片，其中$x$来自于训练数据而非生成器。\n",
    "- $D(x,G(x,z))$：鉴别器网络，给出图像判定为真实图像的概率，其中$x$来自于训练数据，$G(x,z)$来自于生成器。\n",
    "\n",
    "cGAN的目标可以表示为：\n",
    "\n",
    "$$L_{cGAN}(G,D)=E_{(x,y)}[log(D(x,y))]+E_{(x,z)}[log(1-D(x,G(x,z)))]$$\n",
    "\n",
    "该公式是cGAN的损失函数，`D`想要尽最大努力去正确分类真实图像与“假”图像，也就是使参数$log D(x,y)$最大化；而`G`则尽最大努力用生成的“假”图像$y$欺骗`D`，避免被识破，也就是使参数$log(1−D(G(x,z)))$最小化。cGAN的目标可简化为：\n",
    "\n",
    "$$arg\\min_{G}\\max_{D}L_{cGAN}(G,D)$$\n",
    "\n",
    "![1.png](./images/1.png)\n",
    "\n",
    "为了对比cGAN和GAN的不同，我们将GAN的目标也进行了说明：\n",
    "\n",
    "$$L_{GAN}(G,D)=E_{y}[log(D(y))]+E_{(x,z)}[log(1-D(x,z))]$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从公式可以看出，GAN直接由随机噪声$z$生成“假”图像，不借助观测图像$x$的任何信息。过去的经验告诉我们，GAN与传统损失混合使用是有好处的，鉴别器的任务不变，依旧是区分真实图像与“假”图像，但是生成器的任务不仅要欺骗鉴别器，还要在传统损失的基础上接近训练数据。假设cGAN与L1正则化混合使用，那么有:\n",
    "\n",
    "$$L_{L1}(G)=E_{(x,y,z)}[||y-G(x,z)||_{1}]$$\n",
    "\n",
    "进而得到最终目标：\n",
    "\n",
    "$$arg\\min_{G}\\max_{D}L_{cGAN}(G,D)+\\lambda L_{L1}(G)$$\n",
    "\n",
    "图像转换问题本质上其实就是像素到像素的映射问题，pix2pix使用完全一样的网络结构和目标函数，仅更换不同的训练数据集就能分别实现以上的任务。本任务将借助MindSpore框架来实现pix2pix的应用。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 准备环节"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 配置环境文件"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "本教程我们在GPU环境下，使用图模式运行实验。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "```context.set_context(mode=context.GRAPH_MODE, device_target=\"GPU\")```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 准备数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在本教程中，我们将使用[指定数据集](http://efrosgans.eecs.berkeley.edu/pix2pix/datasets/)，该数据集为六种数据集，分别为外墙（facades）、市景（cityscapes）、地图（maps）、昼夜图（night2day）、鞋子图片及对应线条图（edges2shoes）、手包图片及对应线条图（edges2handbags）。其中facades有606张图片，cityscapes有3475张图片，maps有2194张图片，night2day有20120张图片，edges2shoes有50025张图片，edges2handbags有138767张图片。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "每类数据集均保存于./data/datasets文件夹下，如./data/datasets/maps等"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据处理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先为执行过程定义一些配置参数："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_args():  # some parameters\n",
    "    parser = argparse.ArgumentParser(description='config')\n",
    "    parser.add_argument('--train_data_dir', default='../data/maps/train/', type=str)\n",
    "    parser.add_argument('--device_target', default='GPU', choices=['GPU', 'Ascend'], type=str)\n",
    "    parser.add_argument('--train_fakeimg_dir', default='results/fake_img/', type=str)\n",
    "    parser.add_argument('--loss_show_dir', default='results/loss_show', type=str)\n",
    "    parser.add_argument('--ckpt_dir', default='results/ckpt', type=str)\n",
    "    parser.add_argument('--epoch_num', default=200, type=int)\n",
    "    parser.add_argument('--batch_size', default=1, type=int)\n",
    "    parser.add_argument('--beta1', default=0.5, type=float)\n",
    "    parser.add_argument('--beta2', default=0.999, type=float)\n",
    "    return parser.parse_args()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一些参数的说明  \n",
    "train_data_dir：数据训练集文件路径；  \n",
    "device_target：使用设备；  \n",
    "train_fakeimg_dir：训练中存储的假图像文件路径；  \n",
    "loss_show_dir：训练中存储损失图像文件路径；  \n",
    "ckpt_dir：训练中存储checkpoint文件路径；  \n",
    "epoch_num：训练迭代次数,需根据不同数据集具体设置；  \n",
    "batch_size：训练中使用的批量大小,需根据不同数据集具体设置；  \n",
    "beta1：Adam 优化器 beta1；  \n",
    "beta2：Adam 优化器 beta2；  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "论文中对训练轮次数`epoch_num`和输入图像批次`batch_size`根据其数据集不同建议设定不同。参考：  \n",
    "\n",
    "epoch_num:  \n",
    "facades：200  \n",
    "cityscapes：200  \n",
    "maps：200  \n",
    "night2day：17  \n",
    "edges2shoes：15  \n",
    "edges2handbags：15  \n",
    "\n",
    "batch_size：  \n",
    "facades：1  \n",
    "cityscapes：1  \n",
    "maps：1  \n",
    "night2day：4  \n",
    "edges2shoes：4  \n",
    "edges2handbags：4  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 定义`Pix2PixDataset`和`create_train_dataset`函数对训练数据进行处理和增强操作。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "import mindspore\n",
    "from mindspore import dataset as ds\n",
    "import mindspore.dataset.vision.c_transforms as transforms\n",
    "\n",
    "from src.config.pix2pix_config import pix2pix_config as config\n",
    "\n",
    "\n",
    "def get_params():\n",
    "    \"\"\"\n",
    "    Get parameters from images.\n",
    "\n",
    "    Return:\n",
    "        x,y. get image size information.\n",
    "    \"\"\"\n",
    "\n",
    "    new_h = new_w = config.load_size  # config.load_size\n",
    "\n",
    "    x = np.random.randint(0, np.maximum(0, new_w - config.train_pic_size))\n",
    "    y = np.random.randint(0, np.maximum(0, new_h - config.train_pic_size))\n",
    "\n",
    "    return x, y\n",
    "\n",
    "def crop(img, pos, size=config.train_pic_size):\n",
    "    \"\"\"\n",
    "    Crop the images.\n",
    "\n",
    "    Args:\n",
    "        img (list): image.\n",
    "        pos (int): crop position.\n",
    "        size (int): train image size.\n",
    "\n",
    "    Return:\n",
    "        img. output img.\n",
    "    \"\"\"\n",
    "\n",
    "    ow = oh = config.load_size\n",
    "    x1, y1 = pos\n",
    "    tw = th = size\n",
    "    if ow > tw or oh > th:\n",
    "        img = img.crop((x1, y1, x1 + tw, y1 + th))\n",
    "        return img\n",
    "    return img\n",
    "\n",
    "def sync_random_horizontal_flip(input_images, target_images):\n",
    "    \"\"\"\n",
    "    Randomly flip the input images and the target images.\n",
    "\n",
    "    Args:\n",
    "        input_images (list): input original image.\n",
    "        target_images (list): output image after random horizontal flip.\n",
    "\n",
    "   Return:\n",
    "        out_input: random horizontal flip image.\n",
    "        out_target: random horizontal flip image.\n",
    "    \"\"\"\n",
    "\n",
    "    seed = np.random.randint(0, 2000000000)\n",
    "    mindspore.set_seed(seed)\n",
    "    op = transforms.RandomHorizontalFlip(prob=0.5)\n",
    "    out_input = op(input_images)\n",
    "    mindspore.set_seed(seed)\n",
    "    op = transforms.RandomHorizontalFlip(prob=0.5)\n",
    "    out_target = op(target_images)\n",
    "    return out_input, out_target\n",
    "\n",
    "\n",
    "class Pix2PixDataset():\n",
    "    \"\"\"\n",
    "    Define train process_datasets.\n",
    "\n",
    "    Args:\n",
    "        root_dir(str): train dataset path.\n",
    "\n",
    "    Outputs:\n",
    "        a_crop. crop image a.\n",
    "        b_crop. crop image b.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, root_dir):\n",
    "        self.root_dir = root_dir\n",
    "        self.list_files = os.listdir(self.root_dir)\n",
    "        print(self.list_files)\n",
    "        self.list_files.sort(key=lambda x: int(x[:-4]))\n",
    "        print(self.list_files)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.list_files)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img_file = self.list_files[index]\n",
    "        img_path = os.path.join(self.root_dir, img_file)\n",
    "        ab = Image.open(img_path).convert('RGB')\n",
    "        w, h = ab.size\n",
    "        w2 = int(w / 2)\n",
    "\n",
    "        a = ab.crop((w2, 0, w, h))\n",
    "        b = ab.crop((0, 0, w2, h))\n",
    "\n",
    "        a = a.resize((config.load_size, config.load_size))\n",
    "        b = b.resize((config.load_size, config.load_size))\n",
    "\n",
    "        transform_params = get_params()\n",
    "        a_crop = crop(a, transform_params, size=config.train_pic_size)\n",
    "        b_crop = crop(b, transform_params, size=config.train_pic_size)\n",
    "\n",
    "        return a_crop, b_crop\n",
    "\n",
    "\n",
    "def create_train_dataset(dataset, batch_size):\n",
    "    \"\"\"\n",
    "    Create train process_datasets.\n",
    "\n",
    "    Args:\n",
    "        dataset (Class): image processed dataset.\n",
    "        batch_size (int): train dataset size.\n",
    "\n",
    "    Return:\n",
    "        train dataset parameter.\n",
    "    \"\"\"\n",
    "\n",
    "    mean = [0.5 * 255] * 3\n",
    "    std = [0.5 * 255] * 3\n",
    "\n",
    "    trans = [\n",
    "        transforms.Normalize(mean=mean, std=std),\n",
    "        transforms.HWC2CHW()\n",
    "    ]\n",
    "\n",
    "    train_ds = ds.GeneratorDataset(dataset, column_names=[\"input_images\", \"target_images\"], shuffle=False)\n",
    "\n",
    "    train_ds = train_ds.map(operations=[sync_random_horizontal_flip], input_columns=[\"input_images\", \"target_images\"])\n",
    "\n",
    "    train_ds = train_ds.map(operations=trans, input_columns=[\"input_images\"])\n",
    "    train_ds = train_ds.map(operations=trans, input_columns=[\"target_images\"])\n",
    "\n",
    "    train_ds = train_ds.batch(batch_size=batch_size, drop_remainder=True)\n",
    "\n",
    "    return train_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 调用`Pix2PixDataset`和`create_train_dataset`读取测试集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Pix2PixDataset(root_dir='../data/maps/test/')  #选择有图片的文件夹测试\n",
    "ds = create_train_dataset(dataset, batch_size=config.batch_size)\n",
    "print(\"ds:\", ds.get_dataset_size())\n",
    "print(\"ds:\", ds.get_col_names())\n",
    "print(\"ds.shape:\", ds.output_shapes())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds: 54      #测试文件夹里共有54张图片\n",
    "ds: ['input_images', 'target_images']      #names\n",
    "ds.shape: [[1, 3, 256, 256], [1, 3, 256, 256]]   #output_shapes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 创建网络\n",
    "\n",
    "当处理完数据后，就可以来进行网络的搭建了。网络搭建将逐一详细讨论生成器、判别器和损失函数。生成器G用到的是U-net结构，输入的轮廓图$x$编码再解码成真是图片，判别器D用到的是作者自己提出来的条件判别器PatchGAN，判别器D的作用是在轮廓图 $x$的条件下，对于生成的图片$G(x)$判断为假，对于真实判断为真。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 生成器G结构\n",
    "\n",
    "U-Net是德国Freiburg大学模式识别和图像处理组提出的一种全卷积结构。它分为两个部分，其中左侧是由卷积和降采样操作组成的压缩路径，右侧是由卷积和上采样组成的扩张路径，扩张的每个网络块的输入由上一层上采样的特征和压缩路径部分的特征拼接而成。网络模型整体是一个U形的结构，因此被叫做U-Net。和常见的先降采样到低维度，再升采样到原始分辨率的编解码结构的网络相比，U-Net的区别是加入skip-connection，对应的feature maps和decode之后的同样大小的feature maps按通道拼一起，用来保留不同分辨率下像素级的细节信息。\n",
    "![2.png](./images/2.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 定义Unet Skip Connection Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mindspore.nn as nn\n",
    "import mindspore.ops as ops\n",
    "\n",
    "\n",
    "class UNetSkipConnectionBlock(nn.Cell):\n",
    "    \"\"\"\n",
    "    Unet submodule with skip connection.\n",
    "\n",
    "    Args:\n",
    "        outer_nc (int): The number of filters in the outer conv layer.\n",
    "        inner_nc (int): The number of filters in the inner conv layer.\n",
    "        in_planes (int): The number of channels in input images/features.\n",
    "        dropout (bool): Use dropout or not. Default: False.\n",
    "        submodule (Cell): Previously defined submodules.\n",
    "        outermost (bool): If this module is the outermost module.\n",
    "        innermost (bool): If this module is the innermost module.\n",
    "        alpha (float): LeakyRelu slope. Default: 0.2.\n",
    "        norm_mode (str): Specifies norm method. The optional values are \"batch\", \"instance\".\n",
    "\n",
    "    Outputs:\n",
    "        Tensor, output tensor of Unet submodule.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, outer_nc, inner_nc, in_planes=None, dropout=False,\n",
    "                 submodule=None, outermost=False, innermost=False, alpha=0.2, norm_mode='batch'):\n",
    "        super(UNetSkipConnectionBlock, self).__init__()\n",
    "        downnorm = nn.BatchNorm2d(inner_nc)\n",
    "        upnorm = nn.BatchNorm2d(outer_nc)\n",
    "        use_bias = False\n",
    "        if norm_mode == 'instance':\n",
    "            downnorm = nn.BatchNorm2d(inner_nc, affine=False)\n",
    "            upnorm = nn.BatchNorm2d(outer_nc, affine=False)\n",
    "            use_bias = True\n",
    "        if in_planes is None:\n",
    "            in_planes = outer_nc\n",
    "        downconv = nn.Conv2d(in_planes, inner_nc, kernel_size=4, stride=2, padding=1, has_bias=use_bias, pad_mode='pad')\n",
    "        downrelu = nn.LeakyReLU(alpha)\n",
    "        uprelu = nn.ReLU()\n",
    "\n",
    "        if outermost:\n",
    "            upconv = nn.Conv2dTranspose(inner_nc * 2, outer_nc, kernel_size=4, stride=2, padding=1, pad_mode='pad')\n",
    "            down = [downconv]\n",
    "            up = [uprelu, upconv, nn.Tanh()]\n",
    "            model = down + [submodule] + up\n",
    "        elif innermost:\n",
    "            upconv = nn.Conv2dTranspose(inner_nc, outer_nc, kernel_size=4, stride=2, padding=1, has_bias=use_bias, pad_mode='pad')\n",
    "            down = [downrelu, downconv]\n",
    "            up = [uprelu, upconv, upnorm]\n",
    "            model = down + up\n",
    "        else:\n",
    "            upconv = nn.Conv2dTranspose(inner_nc * 2, outer_nc, kernel_size=4, stride=2, padding=1, has_bias=use_bias, pad_mode='pad')\n",
    "            down = [downrelu, downconv, downnorm]\n",
    "            up = [uprelu, upconv, upnorm]\n",
    "\n",
    "            model = down + [submodule] + up\n",
    "            if dropout:\n",
    "                model.append(nn.Dropout(0.5))\n",
    "\n",
    "        self.model = nn.SequentialCell(model)\n",
    "        self.skip_connections = not outermost\n",
    "        self.concat = ops.Concat(axis=1)\n",
    "\n",
    "    def construct(self, x):\n",
    "        out = self.model(x)\n",
    "        if self.skip_connections:\n",
    "            out = self.concat((out, x))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 基于Unet的生成器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import mindspore.nn as nn\n",
    "\n",
    "from src.models.unet_block import UNetSkipConnectionBlock\n",
    "\n",
    "\n",
    "class UNetGenerator(nn.Cell):\n",
    "    \"\"\"\n",
    "    Unet based generator.\n",
    "\n",
    "    Args:\n",
    "        in_planes (int): the number of channels in input images.\n",
    "        out_planes (int): the number of channels in output images.\n",
    "        ngf (int): the number of filters in the last conv layer.Default: 64.\n",
    "        n_layers (int): the number of downsamplings in UNet.Default: 8.\n",
    "        norm_mode (str): Specifies norm method.\n",
    "        dropout (bool): Use dropout or not. Default: False.\n",
    "\n",
    "    Outputs:\n",
    "        Tensor, output tensor.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_planes, out_planes, ngf=64, n_layers=8, norm_mode='bn', dropout=False):\n",
    "        super(UNetGenerator, self).__init__()\n",
    "\n",
    "        # construct unet structure\n",
    "        unet_block = UNetSkipConnectionBlock(ngf * 8, ngf * 8, in_planes=None, submodule=None,\n",
    "                                             norm_mode=norm_mode, innermost=True)\n",
    "        for _ in range(n_layers - 5):\n",
    "            unet_block = UNetSkipConnectionBlock(ngf * 8, ngf * 8, in_planes=None, submodule=unet_block,\n",
    "                                                 norm_mode=norm_mode, dropout=dropout)\n",
    "\n",
    "        # gradually reduce the number of filters from ngf * 8 to ngf\n",
    "        unet_block = UNetSkipConnectionBlock(ngf * 4, ngf * 8, in_planes=None, submodule=unet_block,\n",
    "                                             norm_mode=norm_mode)\n",
    "        unet_block = UNetSkipConnectionBlock(ngf * 2, ngf * 4, in_planes=None, submodule=unet_block,\n",
    "                                             norm_mode=norm_mode)\n",
    "        unet_block = UNetSkipConnectionBlock(ngf, ngf * 2, in_planes=None, submodule=unet_block,\n",
    "                                             norm_mode=norm_mode)\n",
    "        self.model = UNetSkipConnectionBlock(out_planes, ngf, in_planes=in_planes, submodule=unet_block,\n",
    "                                             outermost=True, norm_mode=norm_mode)\n",
    "\n",
    "    def construct(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "原始cGAN的输入是条件x和噪声z两种信息，这里的生成器只使用了条件信息，因此不能生成多样性的结果。因此pix2pix在训练和测试时都使用了dropout，这样可以生成多样性的结果。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 判别器\n",
    "\n",
    "判别器使用的PatchGAN结构，可看做卷积。生成的矩阵中的每个点代表原图的一小块区域（patch）。通过矩阵中的各个值来判断原图中对应每个Patch的真假。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import mindspore.nn as nn\n",
    "from mindspore.ops import Concat\n",
    "\n",
    "from src.config.pix2pix_config import pix2pix_config as config\n",
    "\n",
    "\n",
    "class ConvNormRelu(nn.Cell):\n",
    "    \"\"\"\n",
    "    Convolution fused with BatchNorm/InstanceNorm and ReLU/LackyReLU block definition.\n",
    "\n",
    "    Args:\n",
    "        in_planes (int): Input channel.\n",
    "        out_planes (int): Output channel.\n",
    "        kernel_size (int): Input kernel size. Default: 4.\n",
    "        stride (int): Stride size for the first convolutional layer. Default: 2.\n",
    "        alpha (float): Slope of LackyReLU. Default: 0.2.\n",
    "        norm_mode (str): Specifies norm method. The optional values are \"batch\", \"instance\".\n",
    "        pad_mode (str): Specifies padding mode. The optional values are CONSTANT, REFLECT, SYMMETRIC. Default: CONSTANT.\n",
    "        use_relu (bool): Use relu or not. Default: True.\n",
    "        padding (int): Pad size, if it is None, it will calculate by kernel_size. Default: None.\n",
    "\n",
    "    Outputs:\n",
    "        Tensor, output tensor of module layer.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 in_planes,\n",
    "                 out_planes,\n",
    "                 kernel_size=4,\n",
    "                 stride=2,\n",
    "                 alpha=0.2,\n",
    "                 norm_mode='batch',\n",
    "                 pad_mode='CONSTANT',\n",
    "                 use_relu=True,\n",
    "                 padding=None):\n",
    "        super(ConvNormRelu, self).__init__()\n",
    "        norm = nn.BatchNorm2d(out_planes)\n",
    "        if norm_mode == 'instance':    # Use BatchNorm2d with batchsize=1, affine=False, training=True instead of InstanceNorm2d\n",
    "            norm = nn.BatchNorm2d(out_planes, affine=False)\n",
    "        has_bias = (norm_mode == 'instance')\n",
    "        if not padding:\n",
    "            padding = (kernel_size - 1) // 2\n",
    "        if config.pad_mode == 'REFLECT':\n",
    "            pad_mode = \"REFLECT\"\n",
    "        elif config.pad_mode == \"SYMMETRIC\":\n",
    "            pad_mode = \"SYMMETRIC\"\n",
    "        if pad_mode == 'CONSTANT':\n",
    "            conv = nn.Conv2d(in_planes, out_planes, kernel_size, stride, pad_mode='pad',\n",
    "                             has_bias=has_bias, padding=padding)\n",
    "            layers = [conv, norm]\n",
    "        else:\n",
    "            paddings = ((0, 0), (0, 0), (padding, padding), (padding, padding))\n",
    "            pad = nn.Pad(paddings=paddings, mode=pad_mode)\n",
    "            conv = nn.Conv2d(in_planes, out_planes, kernel_size, stride, pad_mode='pad', has_bias=has_bias)\n",
    "            layers = [pad, conv, norm]\n",
    "        if use_relu:\n",
    "            relu = nn.ReLU()\n",
    "            if alpha > 0:\n",
    "                relu = nn.LeakyReLU(alpha)\n",
    "            layers.append(relu)\n",
    "        self.features = nn.SequentialCell(layers)\n",
    "\n",
    "    def construct(self, x):\n",
    "        output = self.features(x)\n",
    "        return output\n",
    "\n",
    "\n",
    "class Discriminator(nn.Cell):\n",
    "    \"\"\"\n",
    "    Discriminator of Model.\n",
    "\n",
    "    Args:\n",
    "        in_planes (int): Input channel. Default: 3.\n",
    "        ndf (int): the number of filters in the last conv layer. Default: 64.\n",
    "        n_layers (int): The number of ConvNormRelu blocks. Default: 3.\n",
    "        alpha (float): LeakyRelu slope. Default: 0.2.\n",
    "        norm_mode (str): Specifies norm method. The optional values are \"batch\", \"instance\". Default: \"batch\".\n",
    "\n",
    "    Outputs:\n",
    "        Tensor, output tensor of discriminator of model.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_planes=3, ndf=64, n_layers=3, alpha=0.2, norm_mode='batch'):\n",
    "        super(Discriminator, self).__init__()\n",
    "        kernel_size = 4\n",
    "        layers = [\n",
    "            nn.Conv2d(in_planes, ndf, kernel_size, 2, pad_mode='pad', padding=1),\n",
    "            nn.LeakyReLU(config.alpha)\n",
    "        ]\n",
    "        nf_mult = ndf\n",
    "        for i in range(1, n_layers):\n",
    "            nf_mult_prev = nf_mult\n",
    "            nf_mult = min(2 ** i, 8) * ndf\n",
    "            layers.append(ConvNormRelu(nf_mult_prev, nf_mult, kernel_size, 2, alpha, norm_mode, padding=1))\n",
    "        nf_mult_prev = nf_mult\n",
    "        nf_mult = min(2 ** n_layers, 8) * ndf\n",
    "        layers.append(ConvNormRelu(nf_mult_prev, nf_mult, kernel_size, 1, alpha, norm_mode, padding=1))\n",
    "        layers.append(nn.Conv2d(nf_mult, 1, kernel_size, 1, pad_mode='pad', padding=1))\n",
    "\n",
    "        self.features = nn.SequentialCell(layers)\n",
    "        self.concat = Concat(axis=1)\n",
    "\n",
    "    def construct(self, x, y):\n",
    "        x_y = self.concat((x, y))\n",
    "        output = self.features(x_y)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pix2pix的生成器和判别器初始化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mindspore.nn as nn\n",
    "from mindspore.common import initializer as init\n",
    "\n",
    "from src.config.pix2pix_config import pix2pix_config as config\n",
    "from src.models.generator import UNetGenerator\n",
    "from src.models.discriminator import Discriminator\n",
    "\n",
    "\n",
    "def get_generator():\n",
    "    \"\"\"\n",
    "    Return a generator by args.\n",
    "\n",
    "    Returns:\n",
    "        net_generator. initialization generator network.\n",
    "    \"\"\"\n",
    "\n",
    "    net_generator = UNetGenerator(in_planes=config.g_in_planes, out_planes=config.g_out_planes,\n",
    "                                  ngf=config.g_ngf, n_layers=config.g_layers)\n",
    "    for _, cell in net_generator.cells_and_names():\n",
    "        if isinstance(cell, (nn.Conv2d, nn.Conv2dTranspose)):\n",
    "            if config.init_type == 'normal':\n",
    "                cell.weight.set_data(init.initializer(init.Normal(config.init_gain), cell.weight.shape))\n",
    "            elif config.init_type == 'xavier':\n",
    "                cell.weight.set_data(init.initializer(init.XavierUniform(config.init_gain), cell.weight.shape))\n",
    "            elif config.init_type == 'constant':\n",
    "                cell.weight.set_data(init.initializer(0.001, cell.weight.shape))\n",
    "            else:\n",
    "                raise NotImplementedError('initialization method [%s] is not implemented' % config.init_type)\n",
    "        elif isinstance(cell, nn.BatchNorm2d):\n",
    "            cell.gamma.set_data(init.initializer('ones', cell.gamma.shape))\n",
    "            cell.beta.set_data(init.initializer('zeros', cell.beta.shape))\n",
    "    return net_generator\n",
    "\n",
    "def get_discriminator():\n",
    "    \"\"\"\n",
    "    Return a discriminator by args.\n",
    "\n",
    "     Returns:\n",
    "        net_discriminator. initialization discriminator network.\n",
    "    \"\"\"\n",
    "\n",
    "    net_discriminator = Discriminator(in_planes=config.d_in_planes, ndf=config.d_ndf,\n",
    "                                      alpha=config.alpha, n_layers=config.d_layers)\n",
    "    for _, cell in net_discriminator.cells_and_names():\n",
    "        if isinstance(cell, (nn.Conv2d, nn.Conv2dTranspose)):\n",
    "            if config.init_type == 'normal':\n",
    "                cell.weight.set_data(init.initializer(init.Normal(config.init_gain), cell.weight.shape))\n",
    "            elif config.init_type == 'xavier':\n",
    "                cell.weight.set_data(init.initializer(init.XavierUniform(config.init_gain), cell.weight.shape))\n",
    "            elif config.init_type == 'constant':\n",
    "                cell.weight.set_data(init.initializer(0.001, cell.weight.shape))\n",
    "            else:\n",
    "                raise NotImplementedError('initialization method [%s] is not implemented' % config.init_type)\n",
    "        elif isinstance(cell, nn.BatchNorm2d):\n",
    "            cell.gamma.set_data(init.initializer('ones', cell.gamma.shape))\n",
    "            cell.beta.set_data(init.initializer('zeros', cell.beta.shape))\n",
    "    return net_discriminator\n",
    "\n",
    "\n",
    "class Pix2Pix(nn.Cell):\n",
    "    \"\"\"\n",
    "    pix2pix model network.\n",
    "\n",
    "    Args:\n",
    "        discriminator (Cell): a generator.\n",
    "        generator (Cell): a discriminator.\n",
    "\n",
    "    Inputs:\n",
    "        -**reala** - generate real image information.\n",
    "\n",
    "    Outputs:\n",
    "        fakeb, a fake image information.\n",
    "    \"\"\"\n",
    "    def __init__(self, discriminator, generator):\n",
    "        super(Pix2Pix, self).__init__(auto_prefix=True)\n",
    "        self.netd = discriminator\n",
    "        self.netg = generator\n",
    "\n",
    "    def construct(self, reala):\n",
    "        fakeb = self.netg(reala)\n",
    "        return fakeb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "实例化pix2pix生成器和判别器，并打印网络结构。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_generator = get_generator()\n",
    "net_discriminator = get_discriminator()\n",
    "pix2pix = Pix2Pix(generator=net_generator, discriminator=net_discriminator)\n",
    "\n",
    "#遍历每个参数，并打印网络各层名字和属性\n",
    "for m in pix2pix.parameters_and_names():\n",
    "    print(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "('netd.features.0.weight', Parameter (name=netd.features.0.weight, shape=(64, 6, 4, 4), dtype=Float32, requires_grad=True))\n",
    "('netd.features.2.features.0.weight', Parameter (name=netd.features.2.features.0.weight, shape=(128, 64, 4, 4), dtype=Float32, requires_grad=True))\n",
    "('netd.features.2.features.1.moving_mean', Parameter (name=netd.features.2.features.1.moving_mean, shape=(128,), dtype=Float32, requires_grad=False))\n",
    "('netd.features.2.features.1.moving_variance', Parameter (name=netd.features.2.features.1.moving_variance, shape=(128,), dtype=Float32, requires_grad=False))\n",
    "('netd.features.2.features.1.gamma', Parameter (name=netd.features.2.features.1.gamma, shape=(128,), dtype=Float32, requires_grad=True))\n",
    "('netd.features.2.features.1.beta', Parameter (name=netd.features.2.features.1.beta, shape=(128,), dtype=Float32, requires_grad=True))\n",
    "('netd.features.3.features.0.weight', Parameter (name=netd.features.3.features.0.weight, shape=(256, 128, 4, 4), dtype=Float32, requires_grad=True))\n",
    "('netd.features.3.features.1.moving_mean', Parameter (name=netd.features.3.features.1.moving_mean, shape=(256,), dtype=Float32, requires_grad=False))\n",
    "('netd.features.3.features.1.moving_variance', Parameter (name=netd.features.3.features.1.moving_variance, shape=(256,), dtype=Float32, requires_grad=False))\n",
    "('netd.features.3.features.1.gamma', Parameter (name=netd.features.3.features.1.gamma, shape=(256,), dtype=Float32, requires_grad=True))\n",
    "('netd.features.3.features.1.beta', Parameter (name=netd.features.3.features.1.beta, shape=(256,), dtype=Float32, requires_grad=True))  \n",
    "(结构太长，只展示部分)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 连接网络和损失函数\n",
    "\n",
    "MindSpore将损失函数、优化器等操作都封装到了Cell中，因为GAN结构上的特殊性，其损失是判别器和生成器的多输出形式，这就导致它和一般的分类网络不同。所以我们需要自定义`WithLossCell`类，将网络和Loss连接起来。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import mindspore.nn as nn\n",
    "import mindspore.ops as ops\n",
    "from mindspore.ops import functional as opsf\n",
    "import mindspore.ops.operations as opsp\n",
    "from mindspore.parallel._utils import (_get_device_num, _get_gradients_mean, _get_parallel_mode)\n",
    "from mindspore.context import ParallelMode\n",
    "from mindspore.nn.wrap.grad_reducer import DistributedGradReducer\n",
    "from mindspore.nn.loss.loss import LossBase\n",
    "\n",
    "from src.config.pix2pix_config import pix2pix_config as config\n",
    "\n",
    "\n",
    "class SigmoidCrossEntropyWithLogits(LossBase):\n",
    "    \"\"\"\n",
    "    Defining Sigmoid Cross Entropy Loss as Loss Function.\n",
    "\n",
    "    Inputs:\n",
    "        -**data** (Tensor) - Tensor of image data.\n",
    "        -**label** (Tensor) - Tensor of image label.\n",
    "\n",
    "    Outputs:\n",
    "        SigmoidCrossEntropy loss function.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(SigmoidCrossEntropyWithLogits, self).__init__()\n",
    "        self.cross_entropy = opsp.SigmoidCrossEntropyWithLogits()\n",
    "\n",
    "    def construct(self, data, label):\n",
    "        x = self.cross_entropy(data, label)\n",
    "        return self.get_loss(x)\n",
    "\n",
    "class LossD(LossBase):\n",
    "    \"\"\"\n",
    "    Define discriminator loss\n",
    "\n",
    "    args:\n",
    "        reduction (str): Return loss of the samples. Default: \"mean\".\n",
    "\n",
    "    Inputs:\n",
    "        -**pred1** (Tensor) - predict image1.\n",
    "        -**pred0** (Tensor) - predict image0.\n",
    "\n",
    "    Outputs:\n",
    "        discriminator loss.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, reduction=\"mean\"):    # Return the averaging loss of the samples\n",
    "        super(LossD, self).__init__(reduction)\n",
    "        self.sig = SigmoidCrossEntropyWithLogits()\n",
    "        self.ones = ops.OnesLike()\n",
    "        self.zeros = ops.ZerosLike()\n",
    "        self.lambda_dis = config.lambda_dis\n",
    "\n",
    "    def construct(self, pred1, pred0):\n",
    "        loss = self.sig(pred1, self.ones(pred1)) + self.sig(pred0, self.zeros(pred0))\n",
    "        dis_loss = loss * self.lambda_dis\n",
    "        return dis_loss\n",
    "\n",
    "\n",
    "class WithLossCellD(nn.Cell):\n",
    "    \"\"\"\n",
    "    Define WithLossCellD to connect the network and Loss.\n",
    "\n",
    "    Args:\n",
    "        backbone (Cell): backbone of loss network.\n",
    "        loss_fn (Cell): init loss function.\n",
    "\n",
    "    Inputs:\n",
    "        -**reala** (Tensor) - real label a.\n",
    "        -**realb** (Tensor) - real label b.\n",
    "\n",
    "    Outputs:\n",
    "        connected loss function.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, backbone, loss_fn):\n",
    "        super(WithLossCellD, self).__init__(auto_prefix=True)\n",
    "        self.net_discriminator = backbone.net_discriminator\n",
    "        self.net_generator = backbone.net_generator\n",
    "        self._loss_fn = loss_fn\n",
    "\n",
    "    def construct(self, reala, realb):\n",
    "        fakeb = self.net_generator(reala)\n",
    "        pred1 = self.net_discriminator(reala, realb)\n",
    "        pred0 = self.net_discriminator(reala, fakeb)\n",
    "        return self._loss_fn(pred1, pred0)\n",
    "\n",
    "\n",
    "class LossG(LossBase):\n",
    "    \"\"\"\n",
    "    Define generator loss.\n",
    "\n",
    "    Inputs:\n",
    "        -**fakeb** (Tensor) - generate fake image.\n",
    "        -**realb** (Tensor) - real image.\n",
    "        -**pred0** (Tensor) - predict image.\n",
    "\n",
    "    Outputs:\n",
    "        generator loss.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, reduction=\"mean\"):   #reduction=\"mean\": Return the averaging loss of the samples\n",
    "        super(LossG, self).__init__(reduction)\n",
    "        self.sig = SigmoidCrossEntropyWithLogits()\n",
    "        self.l1_loss = nn.L1Loss()\n",
    "        self.ones = ops.OnesLike()\n",
    "        self.lambda_gan = config.lambda_gan\n",
    "        self.lambda_l1 = config.lambda_l1\n",
    "\n",
    "    def construct(self, fakeb, realb, pred0):\n",
    "        loss_1 = self.sig(pred0, self.ones(pred0))\n",
    "        loss_2 = self.l1_loss(fakeb, realb)\n",
    "        loss = loss_1 * self.lambda_gan + loss_2 * self.lambda_l1\n",
    "        return loss\n",
    "\n",
    "\n",
    "class WithLossCellG(nn.Cell):\n",
    "    \"\"\"\n",
    "    Define WithLossCellG to connect the network and Loss.\n",
    "\n",
    "    Args:\n",
    "        backbone (Cell): backbone of loss network.\n",
    "        loss_fn (Cell): init loss function.\n",
    "\n",
    "    Inputs:\n",
    "        -**reala** (Tensor) - real label a.\n",
    "        -**realb** (Tensor) - real label b.\n",
    "\n",
    "    Outputs:\n",
    "        connected loss function.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, backbone, loss_fn):\n",
    "        super(WithLossCellG, self).__init__(auto_prefix=True)\n",
    "        self.net_discriminator = backbone.net_discriminator\n",
    "        self.net_generator = backbone.net_generator\n",
    "        self._loss_fn = loss_fn\n",
    "\n",
    "    def construct(self, reala, realb):\n",
    "        fakeb = self.net_generator(reala)\n",
    "        pred0 = self.net_discriminator(reala, fakeb)\n",
    "        return self._loss_fn(fakeb, realb, pred0)\n",
    "\n",
    "\n",
    "class TrainOneStepCell(nn.Cell):\n",
    "    \"\"\"\n",
    "    Define TrainOneStepCell to encapsulate the training of the discriminator and generator together.\n",
    "\n",
    "    Args:\n",
    "        loss_netd (Cell): loss network of discriminator.\n",
    "        loss_netg (Cell): loss network of generator.\n",
    "        optimizerd (Union[Cell]): optimizer that updates discriminator network parameters.\n",
    "        optimizerg (Union[Cell]): optimizer that updates generator network parameters.\n",
    "        sens (numbers.Number): Input to backpropagation, scaling factor. Default: 1.\n",
    "        auto_prefix (bool): whether auto prefix. Default: True.\n",
    "\n",
    "    Inputs:\n",
    "        -**reala** (Tensor) - real label a.\n",
    "        -**realb** (Tensor) - real label b.\n",
    "\n",
    "    Outputs:\n",
    "        d_res, train generator out output.\n",
    "        g_res, train discriminator output.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, loss_netd, loss_netg, optimizerd, optimizerg, sens=1, auto_prefix=True):\n",
    "        super(TrainOneStepCell, self).__init__(auto_prefix=auto_prefix)\n",
    "        self.loss_net_d = loss_netd\n",
    "        self.loss_net_d.set_grad()\n",
    "        self.loss_net_d.add_flags(defer_inline=True)\n",
    "\n",
    "        self.loss_net_g = loss_netg\n",
    "        self.loss_net_g.set_grad()\n",
    "        self.loss_net_g.add_flags(defer_inline=True)\n",
    "\n",
    "        self.weights_g = optimizerg.parameters\n",
    "        self.optimizerg = optimizerg\n",
    "        self.weights_d = optimizerd.parameters\n",
    "        self.optimizerd = optimizerd\n",
    "\n",
    "        self.grad = ops.GradOperation(get_by_list=True, sens_param=True)\n",
    "        self.sens = sens\n",
    "\n",
    "        # Parallel processing\n",
    "        self.reducer_flag = False\n",
    "        self.grad_reducer_g = opsf.identity\n",
    "        self.grad_reducer_d = opsf.identity\n",
    "        self.parallel_mode = _get_parallel_mode()\n",
    "        if self.parallel_mode in (ParallelMode.DATA_PARALLEL, ParallelMode.HYBRID_PARALLEL):\n",
    "            self.reducer_flag = True\n",
    "        if self.reducer_flag:\n",
    "            mean = _get_gradients_mean()\n",
    "            degree = _get_device_num()\n",
    "            self.grad_reducer_g = DistributedGradReducer(self.weights_g, mean, degree)\n",
    "            self.grad_reducer_d = DistributedGradReducer(self.weights_d, mean, degree)\n",
    "\n",
    "    def set_sens(self, value):\n",
    "        self.sens = value\n",
    "\n",
    "    def construct(self, reala, realb):\n",
    "        \"\"\"Define TrainOneStepCell.\"\"\"\n",
    "        d_loss = self.loss_net_d(reala, realb)\n",
    "        g_loss = self.loss_net_g(reala, realb)\n",
    "        d_sens = ops.Fill()(ops.DType()(d_loss), ops.Shape()(d_loss), self.sens)\n",
    "        d_grads = self.grad(self.loss_net_d, self.weights_d)(reala, realb, d_sens)\n",
    "        d_res = ops.depend(d_loss, self.optimizerd(d_grads))\n",
    "        g_sens = ops.Fill()(ops.DType()(g_loss), ops.Shape()(g_loss), self.sens)\n",
    "        g_grads = self.grad(self.loss_net_g, self.weights_g)(reala, realb, g_sens)\n",
    "        g_res = ops.depend(g_loss, self.optimizerg(g_grads))\n",
    "        return d_res, g_res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练\n",
    "\n",
    "训练分为两个主要部分：训练判别器和训练生成器。\n",
    "\n",
    "- 训练判别器\n",
    "\n",
    "   训练判别器的目的是最大程度地提高判别图像真伪的概率。希望通过提高其随机梯度来更新判别器，所以我们要最大化$log D(x) + log(1 - D(G(z))$的值。\n",
    "\n",
    "- 训练生成器\n",
    "\n",
    "   希望通过最小化$log(1 - D(G(z)))$来训练生成器，以产生更好的虚假图像。\n",
    "\n",
    "   在这两个部分中，分别获取训练过程中的损失，并在每个周期结束时进行统计。\n",
    "\n",
    "下面进行训练："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入各模块\n",
    "import datetime\n",
    "\n",
    "import mindspore.nn as nn\n",
    "from mindspore import context\n",
    "from mindspore import Tensor\n",
    "\n",
    "from models.loss import WithLossCellD, LossD, WithLossCellG, LossG, TrainOneStepCell\n",
    "from models.pix2pix import Pix2Pix, get_generator, get_discriminator\n",
    "from process_datasets.dataset import Pix2PixDataset, create_train_dataset\n",
    "from utils.tools import get_lr\n",
    "from utils.device_adapter import get_device_num\n",
    "from src.config.pix2pix_config import pix2pix_config as arg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 配置信息\n",
    "device_num = get_device_num()\n",
    "context.set_context(mode=context.GRAPH_MODE)\n",
    "\n",
    "# 预处理数据以进行训练\n",
    "dataset = Pix2PixDataset(root_dir=arg.train_data_dir)\n",
    "ds = create_train_dataset(dataset, batch_size=arg.batch_size)\n",
    "steps_per_epoch = ds.get_dataset_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Network\n",
    "net_generator = get_generator()\n",
    "net_discriminator = get_discriminator()\n",
    "pix2pix = Pix2Pix(generator=net_generator, discriminator=net_discriminator)\n",
    "\n",
    "# loss\n",
    "d_loss_fn = LossD()\n",
    "g_loss_fn = LossG()\n",
    "d_loss_net = WithLossCellD(backbone=pix2pix, loss_fn=d_loss_fn)\n",
    "g_loss_net = WithLossCellG(backbone=pix2pix, loss_fn=g_loss_fn)\n",
    "\n",
    "# optimizer\n",
    "d_opt = nn.Adam(pix2pix.netd.trainable_params(), learning_rate=get_lr(),\n",
    "                beta1=arg.beta1, beta2=arg.beta2, loss_scale=1)\n",
    "g_opt = nn.Adam(pix2pix.netg.trainable_params(), learning_rate=get_lr(),\n",
    "                beta1=arg.beta1, beta2=arg.beta2, loss_scale=1)\n",
    "\n",
    "# train net\n",
    "train_net = TrainOneStepCell(loss_netd=d_loss_net, loss_netg=g_loss_net, optimizerd=d_opt, optimizerg=g_opt, sens=1)\n",
    "train_net.set_train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "g_losses = []\n",
    "d_losses = []\n",
    "data_loader = ds.create_dict_iterator(output_numpy=True, num_epochs=arg.epoch_num)\n",
    "for epoch in range(arg.epoch_num):\n",
    "    for i, data in enumerate(data_loader):\n",
    "        start_time = datetime.datetime.now()\n",
    "        input_image = Tensor(data[\"input_images\"])\n",
    "        target_image = Tensor(data[\"target_images\"])\n",
    "        dis_loss, gen_loss = train_net(input_image, target_image)\n",
    "        end_time = datetime.datetime.now()\n",
    "        delta = (end_time - start_time).microseconds\n",
    "        if i % 100 == 0:\n",
    "            print(\"================start===================\")\n",
    "            print(\"Date time: \", start_time)\n",
    "            if arg.run_distribute:\n",
    "                print(\"Device ID :\", str(rank))\n",
    "            print(\"ms per step :\", delta/1000)\n",
    "            print(\"epoch: \", epoch + 1, \"/\", arg.epoch_num)\n",
    "            print(\"step: \", i, \"/\", steps_per_epoch)\n",
    "            print(\"Dloss: \", dis_loss)\n",
    "            print(\"Gloss: \", gen_loss)\n",
    "            print(\"=================end====================\")\n",
    "\n",
    "        d_losses.append(dis_loss.asnumpy())\n",
    "        g_losses.append(gen_loss.asnumpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 循环训练网络，每次迭代，就收集生成器和判别器的损失，以便于后面绘制训练过程中损失函数的图像。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 训练输出"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "================start===================  \n",
    "Date time:  2022-07-06 23:29:31.486978  \n",
    "ms per step : 573.098  \n",
    "epoch:  1 / 15  \n",
    "step:  0 / 34641  \n",
    "Dloss:  0.8268157  \n",
    "Gloss:  96.03406  \n",
    "=================end====================  \n",
    "================start===================  \n",
    "Date time:  2022-07-06 23:29:42.813233  \n",
    "ms per step : 48.086  \n",
    "epoch:  1 / 15  \n",
    "step:  100 / 34641  \n",
    "Dloss:  0.08200404  \n",
    "Gloss:  14.9787245  \n",
    "=================end====================  \n",
    "================start===================  \n",
    "Date time:  2022-07-06 23:29:47.699990  \n",
    "ms per step : 49.513  \n",
    "epoch:  1 / 15  \n",
    "step:  200 / 34641  \n",
    "Dloss:  1.7021327  \n",
    "Gloss:  11.134652  \n",
    "=================end====================  \n",
    "···  \n",
    "================start===================  \n",
    "Date time:  2022-07-07 06:29:31.024008  \n",
    "ms per step : 48.005  \n",
    "epoch:  15 / 15  \n",
    "step:  34400 / 34641  \n",
    "Dloss:  6.0905662e-05  \n",
    "Gloss:  13.91369  \n",
    "=================end====================  \n",
    "================start===================  \n",
    "Date time:  2022-07-07 06:29:35.837478  \n",
    "ms per step : 48.055  \n",
    "epoch:  15 / 15  \n",
    "step:  34500 / 34641  \n",
    "Dloss:  6.202688e-05  \n",
    "Gloss:  17.475426  \n",
    "=================end====================  \n",
    "================start===================  \n",
    "Date time:  2022-07-07 06:29:40.601357  \n",
    "ms per step : 47.031  \n",
    "epoch:  15 / 15  \n",
    "step:  34600 / 34641  \n",
    "Dloss:  1.3251489e-06  \n",
    "Gloss:  17.247965  \n",
    "=================end===================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 结果\n",
    "\n",
    "- 采集本次训练过程中损失值如下："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![3.png](./images/3.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 使用各个数据集训练好的模型分别推理，得到Pix2Pix的图片输出如下："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![4.png](./images/4.png)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
